{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## __Transfer Learning__\n",
        "- Transfer learning refers to a technique in machine learning where a pre-trained model, typically trained on a large dataset, is used as a starting point for solving a different but related task.\n",
        "- It involves using models trained on one problem as a starting point for a related problem.\n",
        "- It is flexible, allowing the use of pre-trained models directly, as feature extraction preprocessing, and integrated into entirely new models.\n",
        "\n"
      ],
      "metadata": {
        "id": "15F8jGyUdtrG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steps to Be Followed:\n",
        "1. Importing the required libraries\n",
        "2. Adding classifier layers\n",
        "3. Preprocessing and feature extraction"
      ],
      "metadata": {
        "id": "LgCmoesorvHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Importing the Required Libraries\n",
        "\n",
        "- The **from tensorflow.keras.utils import load_img** is used to load an image file from the file system.\n",
        "\n",
        "- The **from tensorflow.keras.utils import img_to_array** is used to convert an image loaded with load_img into a NumPy array.\n",
        "\n",
        "- The **from keras.applications.vgg16 import preprocess_input** is used to preprocess the input image array before feeding it to the VGG16 model. VGG16 expects the input images to be preprocessed in a specific way.\n",
        "\n",
        "- The **from keras.applications.vgg16 import VGG16** is used to import the VGG16 model architecture. VGG16 is a popular convolutional neural network model pre-trained on the ImageNet dataset for image classification."
      ],
      "metadata": {
        "id": "cKMViz1Ngp8Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "from tensorflow.keras.utils import load_img\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "5woHmJrJdtrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Adding Classifier Layers\n",
        "- It demonstrates how to load a pre-trained VGG16 model without its classifier layers and then add new custom classifier layers on top of it.\n",
        "- The new model is defined by connecting the output of the pre-trained VGG16 model to a flatten layer, followed by a dense layer with 1024 units and ReLU activation, and finally a dense layer with 10 units and softmax activation for multi-class classification.\n",
        "- The model summary provides an overview of the architecture and layer configurations."
      ],
      "metadata": {
        "id": "K_860Ql8kwv-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "\n",
        "model = VGG16(include_top=False, input_shape=(300, 300, 3))\n",
        "flat1 = Flatten()(model.layers[-1].output)\n",
        "class1 = Dense(1024, activation='relu')(flat1)\n",
        "output = Dense(10, activation='softmax')(class1)\n",
        "\n",
        "model = Model(inputs=model.inputs, outputs=output)\n",
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 300, 300, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 300, 300, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 300, 300, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 150, 150, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 150, 150, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 150, 150, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 75, 75, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 75, 75, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 75, 75, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 75, 75, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 37, 37, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 37, 37, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 37, 37, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 37, 37, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 18, 18, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 41472)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              42468352  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 57,193,290\n",
            "Trainable params: 57,193,290\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h95JEG-VdtrM",
        "outputId": "943b0380-f982-4dcc-ba94-fd71ce47573e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "- Running the example defines the new model ready for training and summarizes the model architecture.\n",
        "- We have flattened the output of the last pooling layer and added our new fully connected layers.\n",
        "-The weights of the VGG16 model and the weights for the new model will all be trained together on the new dataset."
      ],
      "metadata": {
        "id": "E3tLfn7WdtrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Preprocessing and Feature Extraction\n",
        "- The image is loaded from a file and preprocessed to meet the input requirements of the VGG16 model (resizing, converting to a numpy array, and reshaping).\n",
        "\n",
        "- The modified model is used to predict and extract features from the input image, resulting in a feature vector with a specific shape."
      ],
      "metadata": {
        "id": "3DWAS8owlYbP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "image = load_img('dog.jpg', target_size=(224, 224))\n",
        "image = img_to_array(image)\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "image = preprocess_input(image)\n",
        "model = VGG16()\n",
        "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "features = model.predict(image)\n",
        "print(features.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467096/553467096 [==============================] - 5s 0us/step\n",
            "1/1 [==============================] - 1s 722ms/step\n",
            "(1, 4096)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn9hQjZCdtrN",
        "outputId": "e1f032bc-a048-493d-dba5-8d9dd2a65205"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "\n",
        "- The VGG16 model weights are downloaded and loaded successfully, and the extracted features from the input image have a shape of (1, 4096)."
      ],
      "metadata": {
        "id": "Jwvk_m95mR7D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "HfLnQBrHdtrO"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}